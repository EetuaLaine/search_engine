{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is for developing and experimenting with functions related to the search engine project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse a .pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(input_file):\n",
    "    i_f = open(input_file,'rb')\n",
    "    resMgr = PDFResourceManager()\n",
    "    retData = io.StringIO()\n",
    "    TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())\n",
    "    interpreter = PDFPageInterpreter(resMgr,TxtConverter)\n",
    "    for page in PDFPage.get_pages(i_f):\n",
    "        interpreter.process_page(page)\n",
    " \n",
    "    txt = retData.getvalue()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess parsed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "\n",
    "def preprocess_document(document):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    stop_words = stopwords.words('english')\n",
    "    number_token_name = \"special_number_token\"\n",
    "    \n",
    "    tokens = word_tokenize(document)\n",
    "    tokens = [t for t in tokens if t.isalnum() and len(t) > 1 and t not in stop_words]\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    tokens = [number_token_name if t.isnumeric() else t for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine vocabulary\n",
    "\n",
    "Sometimes it occurs that words get cut into two tokens because there is a dash and a newline somewhere in the middle of the word. This results in some noisy tokens being created. Additionally, there are several tokens, which contain both numeric and alpha characters. These could be further examined and parsed with some regex manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aefb5b0756d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Corpus size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(set(tokens)), \"Corpus size:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.Series(tokens).value_counts()\n",
    "\n",
    "print(counts[:60])\n",
    "print(\"-\" * 100)\n",
    "print(counts[-60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Drop the first five elements to better see the distribution of tokens\n",
    "plt.hist([counts.values[5:]], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring together the components into a function that preprocesses a pdf document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pdf_document(pdf_path, stemmer=None, stop_word_list=None, NUMBER_TOKEN_NAME=None):\n",
    "    # Parse pdf\n",
    "    parsed_text = pdf_to_text(pdf_path)\n",
    "    \n",
    "    if stemmer is None:\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "    \n",
    "    if stop_word_list is None:\n",
    "        stop_word_list = stopwords.words('english')\n",
    "    \n",
    "    if NUMBER_TOKEN_NAME is None:\n",
    "        NUMBER_TOKEN_NAME = \"special_number_token\"\n",
    "\n",
    "    preprocessed_document = preprocess_document(parsed_text)\n",
    "    \n",
    "    return preprocessed_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple index for a document\n",
    "\n",
    "This function creates an index for a list, which is essentially a dictionary containing the counts for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(preprocessed_document):\n",
    "    return pd.Series(preprocessed_document).value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine to obtain a function, which indexes a pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_pdf_document(input_pdf):\n",
    "    preprocessed_file = preprocess_pdf_document(input_pdf)\n",
    "    return create_index(preprocessed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity between two indices\n",
    "\n",
    "This function computes the intersection of the value counts divided by the minimum sum of value counts. Under the assumption that one index is significantly smaller than the two, this should quickly give a score based on how many of the token occurrences in the smaller document are found in the larger document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_similarity(idx1, idx2):\n",
    "    sum_1 = sum(idx1.values())\n",
    "    sum_2 = sum(idx2.values())\n",
    "    \n",
    "    intersection = 0\n",
    "    \n",
    "    if sum_1 < sum_2:\n",
    "        min_idx = idx1\n",
    "        max_idx = idx2\n",
    "        min_sum = sum_1\n",
    "    else:\n",
    "        min_idx = idx2\n",
    "        max_idx = idx1\n",
    "        min_sum = sum_2\n",
    "    \n",
    "    for k, v in min_idx.items():\n",
    "        intersection += min(max_idx.get(k, 0), v)\n",
    "    \n",
    "    return intersection / min_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over pdf documents, compute indices and write them to a json file as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_indices(path_to_files, output_path=\"./test_data/indices.json\"):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for file in os.listdir(path_to_files):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".pdf\"): \n",
    "            index = index_pdf_document(os.path.join(path_to_files, filename))\n",
    "            result.append({\"file_name\": filename, \"index\": index})\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "test_path = \"./test_data\"\n",
    "compute_indices(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write function that retrieves the indices from the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_indices_from_json(indices_path=None):\n",
    "    try:\n",
    "        with open(indices_path) as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"JSON loading failed with exception:\", e)\n",
    "        return None\n",
    "\n",
    "json_path = \"./test_data/indices.json\"\n",
    "    \n",
    "    \n",
    "indices = read_indices_from_json(indices_path=json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the most relevant documents given a query string\n",
    "\n",
    "This function takes the similarity function as an argument, and also takes a function as an argument which retrieves the documents in a list where elements are in the form {\"file_name\": example_file.pdf, \"index\": {\"example\": 1, \"token\": 1}}. This way the indices can later be obtained from eg. local json files, an S3 bucket or MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swedbank_annual_2017.pdf',\n",
       " 'danske_bank_Outlook_January 2019.pdf',\n",
       " 'swedbank_mortgage_2018.pdf']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar_documents(query_string, compute_similarity, get_indices, n=3, \n",
    "                           indices_path=\"./test_data/indices.json\"):\n",
    "    \n",
    "    preprocessed_query = preprocess_document(query_string)\n",
    "    query_index = create_index(preprocessed_query)\n",
    "    \n",
    "    indices = get_indices(indices_path=indices_path)\n",
    "    \n",
    "    similarities = [compute_similarity(index[\"index\"], query_index) for index in indices]\n",
    "    \n",
    "    zipped = sorted([(idx, sim) for idx, sim in enumerate(similarities)], key=lambda t: -t[1])\n",
    "    \n",
    "    indices_to_return = [t[0] for t in zipped[:n]]\n",
    "        \n",
    "    return [indices[idx][\"file_name\"] for idx in indices_to_return]\n",
    "\n",
    "most_similar_documents(\"what is the revenue of danske bank\", index_similarity, read_indices_from_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence embedding\n",
    "\n",
    "Sentence embeddings are meant to transform sentences and documents from sequences of tokens into a d-dimensional vector space. A successful embedding algorithm maps semantically similar inputs close to each other in the embedding space wrt. some similarity or distance function.\n",
    "\n",
    "Sentence embeddings could be utilized in this project to, for instance, map all of the documents into a vector space in an offline manner. When the user sends a query, the query would be mapped into that vector space. The service could then return documents with the greatest similarity to the query vector.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Why do this instead of / in addition to the existing lexical model? One reason is that two words or sentences can be semantically similar while being disjoint in terms of exact word matches. As an example:\n",
    "\n",
    "- A king holds dominion over Sweden.\n",
    "- A monarch rules in Spain.\n",
    "\n",
    "These sentences are semantically very close to each other. However, the current similarity function would most likely assign them a similarity of 0 due to stop word removal. An effective sentence embedding model would ideally produce sentence vectors for these sentences, which are similar on a given similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-1.76214278e-01  1.20601267e-01 -2.93623865e-01 -2.29857773e-01\n",
      " -8.22927803e-02  2.37709224e-01  3.39985371e-01 -7.80964255e-01\n",
      "  1.18127957e-01  1.63374022e-01 -1.37714952e-01  2.40282476e-01\n",
      "  4.25125450e-01  1.72417805e-01  1.05279475e-01  5.18164098e-01\n",
      "  6.22214638e-02  3.99286091e-01 -1.81652337e-01 -5.85578680e-01\n",
      "  4.49717082e-02 -1.72750488e-01 -2.68443465e-01 -1.47386283e-01\n",
      " -1.89218074e-01  1.92150757e-01 -3.83842677e-01 -3.96006912e-01\n",
      "  4.30648983e-01 -3.15319538e-01  3.65949363e-01  6.05158247e-02\n",
      "  3.57325971e-01  1.59736261e-01 -3.00983787e-01  2.63250351e-01\n",
      " -3.94311160e-01  1.84855491e-01 -3.99549007e-01 -2.67889708e-01\n",
      " -5.45117259e-01 -3.13406922e-02 -4.30644006e-01  1.33278042e-01\n",
      " -1.74793795e-01 -4.35465485e-01 -4.77378786e-01  7.12556243e-02\n",
      " -7.37002566e-02  5.69137096e-01 -2.82579750e-01  5.24976403e-02\n",
      " -8.20008039e-01  1.98297009e-01  1.69511795e-01  2.71779984e-01\n",
      "  2.64610976e-01 -2.55738832e-02 -1.74096316e-01  1.63314313e-01\n",
      " -3.95260751e-01 -3.17558497e-02 -2.62556016e-01  3.52754414e-01\n",
      "  3.01434964e-01 -1.47197753e-01  2.10075647e-01 -1.84010670e-01\n",
      " -4.12896276e-01  4.14775848e-01 -1.89769104e-01 -1.35482043e-01\n",
      " -3.79272312e-01 -4.68023829e-02 -3.33599970e-02  9.00395438e-02\n",
      " -3.30132663e-01 -3.87318768e-02  3.75082314e-01 -1.46996692e-01\n",
      "  4.34959769e-01  5.38325846e-01 -2.65445501e-01  1.64445817e-01\n",
      "  4.17078167e-01 -4.72507700e-02 -7.48731345e-02 -4.26260740e-01\n",
      " -1.96994483e-01  6.10317960e-02 -4.74262983e-01 -6.48334622e-01\n",
      "  3.71462315e-01  2.50956774e-01  1.22529916e-01  8.88766423e-02\n",
      " -1.06724367e-01  5.33983111e-02  9.74503607e-02 -3.46655846e-02\n",
      " -1.02882698e-01  2.32288912e-01 -2.53740042e-01 -5.13112307e-01\n",
      "  1.85216039e-01 -3.04357976e-01 -3.55211310e-02 -1.26975432e-01\n",
      " -7.71633834e-02 -5.15330017e-01 -2.28072092e-01  2.03345381e-02\n",
      "  7.38176927e-02 -1.52558416e-01 -4.00837928e-01 -2.47749314e-01\n",
      "  3.97470266e-01 -2.60260642e-01  2.50906110e-01  1.68229088e-01\n",
      "  1.33900344e-01 -2.10832953e-02 -4.70035493e-01  4.78850096e-01\n",
      "  2.80345500e-01 -4.64546859e-01  3.21746945e-01  2.34207317e-01\n",
      "  2.45772228e-01 -4.71482128e-01  5.00401437e-01  4.10189986e-01\n",
      "  5.15216947e-01  2.62549400e-01  2.11591907e-02 -3.89687419e-01\n",
      " -2.41742834e-01 -2.14834690e-01 -8.62651616e-02 -1.65323660e-01\n",
      " -5.21892980e-02  3.41874719e-01  4.50314105e-01 -3.06973636e-01\n",
      " -2.02294320e-01  6.85521662e-01 -5.33892512e-01  3.58471245e-01\n",
      "  1.45286500e-01 -7.07054213e-02 -1.50529325e-01 -8.56281146e-02\n",
      " -7.67849758e-02  1.89544469e-01 -1.04067631e-01  5.33543587e-01\n",
      " -5.27886987e-01  2.42330804e-02 -2.64347851e-01 -2.23186463e-01\n",
      " -3.81208777e-01  7.59913400e-02 -4.64485198e-01 -3.36549133e-01\n",
      "  4.21229720e-01  1.07479222e-01  1.90457985e-01  2.89481296e-03\n",
      " -1.08513728e-01  1.53545648e-01  3.16023231e-01 -2.70838235e-02\n",
      " -5.40594459e-01  8.97289887e-02 -1.15549669e-01  3.97803724e-01\n",
      " -4.97683346e-01 -2.84893185e-01  4.99862880e-02  3.61279637e-01\n",
      "  6.90535545e-01  1.46821603e-01  1.73396692e-01 -1.74582317e-01\n",
      " -3.15702409e-01  6.73001409e-02  2.17249840e-01  9.78538170e-02\n",
      " -1.29472479e-01 -1.86929673e-01  1.34878263e-01 -1.53885275e-01\n",
      "  7.44716972e-02 -1.85536012e-01 -2.80628204e-01 -1.14144415e-01\n",
      "  4.12249506e-01  6.39495477e-02 -1.45715252e-01 -9.82061252e-02\n",
      " -1.33081883e-01 -1.88410848e-01 -2.84841619e-02 -3.49510275e-02\n",
      "  3.34260240e-02  6.98897168e-02  1.90354377e-01 -2.96724170e-01\n",
      "  2.64714542e-03  1.09141104e-01  1.70892589e-02  2.60589302e-01\n",
      "  3.29038352e-01 -6.61561415e-02  2.39665404e-01 -2.26194829e-01\n",
      " -3.36866863e-02  1.49400309e-01 -3.21265221e-01 -2.68578082e-01\n",
      "  5.72631657e-01 -4.92308408e-01  2.00666755e-01 -3.49261642e-01\n",
      " -2.89888419e-02  6.09010458e-01 -5.72333276e-01  2.35000312e-01\n",
      "  6.47154357e-03 -3.14950570e-02  2.78108325e-02 -3.90340686e-01\n",
      " -2.08949938e-01 -3.04452777e-01 -7.20195696e-02 -8.29839557e-02\n",
      "  3.73792648e-01  7.38940984e-02 -2.21074298e-02  9.88139287e-02\n",
      " -1.51426762e-01 -1.40430883e-01  2.26018012e-01  2.76089996e-01\n",
      " -8.87749642e-02 -1.12815924e-01 -2.66286194e-01  2.77834564e-01\n",
      " -4.75611761e-02  6.71006665e-02 -2.78586876e-02 -2.39993166e-02\n",
      "  2.51708627e-01  4.68793720e-01 -5.39325655e-01  1.10598378e-01\n",
      " -3.44947278e-01  4.15989935e-01  7.28482828e-02 -3.19647431e-01\n",
      "  4.90374446e-01 -7.30342697e-03 -2.64241570e-03  9.63711143e-01\n",
      "  3.23885083e-01 -7.79618919e-02 -2.37589151e-01  2.34038278e-01\n",
      " -3.16054195e-01 -1.65646034e-03 -1.09070659e+00  3.38409126e-01\n",
      "  4.70605530e-02  1.07435435e-01 -2.06672490e-01  4.26421640e-03\n",
      " -1.38497469e-03 -5.31455457e-01 -2.75648266e-01 -1.64648533e-01\n",
      " -3.42916340e-01 -4.26118821e-01  6.01812243e-01  4.55971867e-01\n",
      " -2.72702008e-01 -3.45807448e-02  2.62752503e-01 -6.34191837e-03\n",
      "  2.79631019e-01 -2.53558993e-01 -1.68626338e-01  3.82933579e-02\n",
      "  2.07763270e-01 -4.31525975e-01 -7.24000633e-02 -1.26854420e-01\n",
      "  2.07030345e-02  5.74441314e-01  3.54672372e-01  9.28304568e-02\n",
      "  6.70507401e-02  1.11520603e-01 -1.86511986e-02  4.62351948e-01\n",
      "  2.72504508e-01 -3.60473990e-01  5.29415190e-01 -1.00290333e-03\n",
      " -8.81362483e-02  1.49975628e-01  5.25861233e-02  4.63517785e-01\n",
      " -3.96831393e-01  2.42640525e-01 -2.08912224e-01  3.65672141e-01\n",
      " -4.73097549e-04  5.33963084e-01 -1.97879642e-01  3.11583221e-01\n",
      " -6.96714938e-01 -4.29500878e-01 -4.49359477e-01 -2.71369237e-02\n",
      " -6.98710382e-02  2.06174463e-01 -1.57107577e-01  4.43521023e-01\n",
      " -6.74266443e-02 -3.00924122e-01  5.14859259e-01  3.36029410e-01\n",
      "  6.63375854e-02 -1.15235180e-01 -2.95979008e-02  2.79471964e-01\n",
      " -3.48199755e-02 -7.29323477e-02 -4.58472632e-02  1.54262513e-01\n",
      "  8.09356034e-01  5.20327926e-01 -4.02114600e-01 -3.23150940e-02\n",
      " -1.10364050e-01  7.50504509e-02 -1.51098654e-01  8.45739722e-01\n",
      " -1.80844069e-01  3.22573483e-01  1.04708195e-01  3.19663823e-01\n",
      " -1.55085266e-01  1.69236526e-01 -2.56996483e-01  2.01208994e-01\n",
      "  1.77393064e-01 -2.74333179e-01 -3.36944044e-01  5.02356708e-01\n",
      " -1.18356973e-01 -2.01167092e-01 -5.36485732e-01 -7.69808814e-02\n",
      "  1.15379458e-02 -2.36464366e-01 -2.98770443e-02  1.31366953e-01\n",
      "  2.94184357e-01  9.90913436e-02 -5.43897748e-01  1.40812993e-01\n",
      "  3.66998971e-01  5.04860282e-02  1.99122518e-01 -2.80674577e-01\n",
      "  4.34192508e-01 -1.40274972e-01  5.78048825e-01  1.77715853e-01\n",
      "  8.98363516e-02  3.29651803e-01  6.13007806e-02 -3.24933499e-01]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 0.3220875  -0.00123935  0.179374   -0.36919153 -0.06460244  0.09153676\n",
      "  0.24119124 -0.2949423   0.07728967  0.11577015 -0.04479996  0.17928252\n",
      "  0.1475357   0.21511672  0.36810777  0.20910932  0.27194235  0.3488006\n",
      " -0.5725191  -0.18253209  0.4448957   0.27452943  0.0426628  -0.07683573\n",
      "  0.1868916   0.4496504  -0.16932607 -0.24896336 -0.20479248  0.40285033\n",
      " -0.21019249  0.03775693  0.07848553  0.1284847   0.02593076  0.47155964\n",
      "  0.17853807 -0.07379743  0.08130757 -0.23328729 -0.49801236 -0.04135702\n",
      " -0.12094591  0.17028989 -0.19154096 -0.38459828 -0.7747913  -0.10622744\n",
      " -0.23044892  0.40241468 -0.8745088   0.23853725 -0.47129872  0.21262197\n",
      "  0.3340936  -0.24153996 -0.14835082 -0.14513579 -0.3483094  -0.08349234\n",
      " -0.6909727  -0.29845265 -0.12230489  0.07482615 -0.18775608 -0.37546536\n",
      "  0.21369499 -0.10096412 -0.12234426  0.3143151  -0.23989977  0.22460775\n",
      "  0.03996019  0.3603483  -0.56638056  0.21883522  0.11020324 -0.10870837\n",
      "  0.07084084 -0.02608172  0.18370332  0.08465943 -0.2047822  -0.24435617\n",
      " -0.08180563 -0.01903093 -0.03591374  0.02398454 -0.28558564  0.07374801\n",
      " -0.29744223 -0.87717843  0.47101948 -0.04940471  0.363945    0.48264396\n",
      "  0.01564637  0.03558928 -0.26203    -0.11218469  0.02411048  0.37477806\n",
      " -0.09897307 -0.09851837  0.15000835  0.00689545 -0.1265248  -0.31598923\n",
      "  0.31449515 -0.2942563  -0.2694104   0.20221186  0.14329885 -0.19584621\n",
      " -0.34104478 -0.0317276   0.7365028   0.31923497  0.24381329  0.30732602\n",
      "  0.09933212  0.19010925 -0.10694525  0.05178652  0.03233417 -0.10314634\n",
      "  0.26499197  0.31206423  0.43152606 -0.64261204  0.08409566 -0.04327356\n",
      " -0.04991193 -0.12718575  0.1378918   0.0130622   0.34383234  0.09234281\n",
      " -0.0992276  -0.52159935  0.2584227  -0.01057149 -0.0047818   0.03938879\n",
      "  0.19086085  0.32933894 -0.24345145 -0.07328311 -0.39280042  0.14541799\n",
      "  0.32839534 -0.0418461   0.0740712  -0.73860544 -0.09075997  0.15802337\n",
      " -0.0978004  -0.21605973 -0.30027452  0.23236585  0.0107247   0.49570462\n",
      "  0.04974842  0.2993141  -0.05382262  0.353281    0.34191757  0.49667245\n",
      " -0.48605233 -0.19098872  0.8154573   0.22962631 -0.32077798 -0.32726723\n",
      " -0.3677173   0.34521177 -0.02620163 -0.14315063  0.10648439 -0.24638051\n",
      " -0.09366623  0.17198633 -0.08508801  0.20120281 -0.0587921  -0.34021\n",
      " -0.19565329  0.28280866  0.20124294 -0.08207249  0.09779146 -0.26374996\n",
      "  0.12176564 -0.01041472 -0.43859816  0.11058248  0.48010385 -0.10981948\n",
      " -0.6375458   0.29336804 -0.19207643  0.4653698   0.27042025  0.19388495\n",
      "  0.17379029 -0.30077016 -0.02751192 -0.02291252  0.36784622  0.02492197\n",
      "  0.5370552   0.18851228 -0.13344432  0.0891735   0.05542925 -0.24818325\n",
      " -0.04199776  0.05767372 -0.1827882  -0.41686457  0.1607058  -0.46362525\n",
      "  0.11769214 -0.37706897  0.02960376  0.69256115 -0.48308936  0.2112837\n",
      "  0.18214521 -0.18429591  0.06817656 -0.02460911 -0.19073611 -0.06736989\n",
      " -0.5670072  -0.23929314 -0.08497223  0.03093972  0.31079933  0.12916256\n",
      "  0.05248233 -0.33449814  0.18810128  0.23547147 -0.00183473  0.45361575\n",
      "  0.24885106 -0.05641069 -0.29774597 -0.43511745 -0.0796943  -0.17670183\n",
      " -0.13347085  0.19382721  0.22002602 -0.11057527  0.26473764 -0.2717907\n",
      "  0.03410888 -0.47714412  0.44719055 -0.05570415  0.39643756  0.27483252\n",
      "  0.333056   -0.1089024   0.27888173  0.21596931 -0.05252272 -0.35867536\n",
      " -0.69062895  0.03960207  0.00652772 -0.01095359 -0.10027675  0.04770041\n",
      " -0.34146932 -0.16714166  0.07136441 -0.18078472 -0.30248454 -0.6842873\n",
      " -0.0959285  -0.21411107 -0.6552439   0.5675645   0.2694669  -0.00190048\n",
      "  0.8618065   0.16771573  0.03102774 -0.26773062 -0.07830299 -0.48510867\n",
      " -0.26737216 -0.33354267 -0.5738251   0.3567827   0.08993585 -0.1305719\n",
      " -0.15136486 -0.06124157 -0.13037075  0.5585608   0.61417496 -0.04804043\n",
      " -0.06388588  0.083906   -0.25143683 -0.04359837 -0.18525814  0.0469337\n",
      " -0.34380874 -0.09738508  0.16833627  0.07526844  0.17694497  0.1772718\n",
      " -0.03423426  0.14993562 -0.13773179 -0.20949706 -0.6127284   0.3781397\n",
      "  0.39018258 -0.08359328  0.03152131  0.13122398  0.38826072  0.2184424\n",
      "  0.09724298  0.42089346 -0.32641247 -0.2693343  -0.39095104 -0.22648646\n",
      " -0.32020706 -0.16287428 -0.03581612  0.3637389   0.18583275 -0.02914033\n",
      " -0.4657792   0.29168895  0.37251273 -0.23726615  0.0033862   0.4154096\n",
      "  0.03300444  0.45003965 -0.08159252  0.33990338  0.244979    0.02352408\n",
      " -0.14643069 -0.12644538  0.31128624 -0.15182637  0.01009383  0.49108517\n",
      "  0.14362408  0.11589038 -0.2323699   0.2475174   0.18364488 -0.24836826\n",
      " -0.11220937 -0.2311336   0.08428954 -0.24378638  0.13307285  0.42355683\n",
      "  0.33348396 -0.34370145  0.03443633  0.18795514  0.2003717  -0.05355966\n",
      "  0.28485298  0.07176547  0.05487151 -0.08103786  0.27076894  0.11700239]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 0.5897936  -0.23598313 -0.25411707  0.0031161  -0.08485702 -0.26799747\n",
      " -0.07506666 -0.30021352  0.0515166   0.16585301  0.2607676   0.3825636\n",
      "  0.4373288  -0.09301975 -0.26568794 -0.09716291 -0.4809605   0.11878293\n",
      "  0.1367549   0.04712053 -0.23696494 -0.5233238  -0.01631863  0.06127287\n",
      " -0.74332976 -0.11898893 -0.7886527  -0.48108825  0.10314921 -0.32372436\n",
      "  0.8144374  -0.39774558 -0.50315565 -0.79724556 -0.6324822   0.3232096\n",
      " -0.38419405 -0.11186665 -0.13243575  0.02069706 -0.14309537 -0.03701162\n",
      "  0.06116635  0.16332881 -0.1117431   0.25234213 -1.0464075  -0.37252355\n",
      "  0.1560199  -0.29991582  0.19883849  0.23433433 -0.370258    0.31733593\n",
      "  0.84428596  0.06977705  0.0327364   0.09948364 -0.31141332  0.5051772\n",
      "  0.00309262  0.38013682  0.0458275   0.00633364 -0.00142957 -0.13568674\n",
      " -0.07611424 -0.258443   -0.8022129   0.550859   -0.09124396 -0.21782048\n",
      " -0.788109   -0.51183814  0.46672523  0.55274725 -0.3712474  -0.18645324\n",
      "  0.3585697  -0.19586372  0.18042518 -0.42548895 -0.09681364 -0.05536783\n",
      "  0.5248929   0.24481143  0.01934637 -0.29637927 -0.12777843 -0.30534947\n",
      "  0.4534936   0.07469147 -0.07061682  0.26243016  0.3738395   0.14306353\n",
      "  0.00127892 -0.4177607  -0.24014105 -0.25093487  0.34843758  0.31144068\n",
      "  0.08087335 -0.57640535  0.54085296 -0.01802242 -0.12959822 -0.07399689\n",
      "  0.39369795  0.64883834 -0.02029961 -0.56655544  0.29675975  0.5200023\n",
      "  0.21538727  0.10369637  0.06199219  0.01896275 -0.15269189 -1.0642658\n",
      "  0.76149637  0.20734413  0.44718924  0.14493969  0.6580229  -0.09440961\n",
      " -0.23316358  0.42157075  0.11957613 -0.32571056  0.16425514 -0.49508664\n",
      " -0.19516122 -0.5618324  -0.14933263  0.6109409  -0.1789792  -0.01805549\n",
      " -0.596405    0.04918597  0.15347768 -0.4282939   0.732953   -0.3529111\n",
      " -0.11159643  0.06127818 -0.29704425  0.439666   -0.09660374  0.6557943\n",
      " -0.6140334   0.02576625  0.43827465  0.01733224 -0.4000226  -0.08178332\n",
      " -0.3712699   0.08230294 -0.13104393 -0.5326109  -0.29928336  0.6993656\n",
      " -0.04398751 -0.15703014  0.09794111 -0.03017486 -0.10002708  0.19996582\n",
      " -0.48188528  0.17949152  0.5656602  -0.11954827 -0.696373    0.05259687\n",
      " -0.0054961   0.16739364 -0.3169286  -0.09747537  0.33193654  0.47199592\n",
      "  0.12653987  0.19130938  0.42949054  0.5529122   0.31463298 -0.31433082\n",
      " -0.41508678  0.32897735  0.35702705 -0.1920964   0.22239423 -0.4871789\n",
      "  0.34091544 -0.22137429 -0.12667584  0.21120809 -0.31347856  0.84689397\n",
      "  0.20112674 -0.4259877   0.51315707 -1.2351414   0.76971775 -0.17414278\n",
      " -0.02181109 -0.03568656 -1.1059493  -0.57206553  0.05585249  0.12461465\n",
      " -0.45065895  0.06428982 -0.16033864  0.39932916 -0.10322874 -0.02025511\n",
      " -0.18010509  0.06234778 -0.02188908 -0.1579539   0.28316957  0.02385289\n",
      "  0.03098102 -0.07853299  0.29896542 -0.06237337  0.54986775  0.17862345\n",
      "  0.2116474   0.4448335   0.04890762 -0.16238119 -0.22669904  0.18872021\n",
      "  0.07943621  0.13597578 -0.18484445  1.1135509   0.8280952  -0.3120272\n",
      "  0.09506006  0.05096094  0.3880491   0.25000495  0.5584859   0.31088743\n",
      " -0.05318598 -0.07675344  0.152823    0.09189982 -0.01429147  0.6657542\n",
      " -0.03346028 -0.44703522  0.80067486 -0.4799277   0.17478187 -0.3056388\n",
      "  0.55365235  0.42380932  0.48674306 -0.49677983 -0.45194784 -0.9556308\n",
      " -0.20709927 -0.22605765 -0.00999204  0.98797673  0.58807725  0.08305465\n",
      " -0.5578133   0.21136844 -0.36072266  0.52668476  0.3398359  -0.15756176\n",
      "  0.00423772 -0.05354509 -0.5777674   0.55951035 -0.05747166  0.16837676\n",
      "  0.37946847 -0.25776416  0.08421459 -0.15229917 -0.03280774  0.10083874\n",
      " -0.41858327 -0.4449902  -0.29309914  0.61442083  0.0854815  -0.06349538\n",
      " -0.6152554   0.79544073 -0.24058384  0.20638876 -0.512526    0.6312012\n",
      "  0.3674432  -0.4400989   0.4691398   0.2308772  -0.13737997  0.21696883\n",
      "  0.40043244 -0.0249064  -1.1396754   0.026539   -0.32730204  0.09984177\n",
      "  0.05725672 -0.8472212   0.06451976  0.45698023  0.6356301   0.4518563\n",
      " -0.27519065  0.2134616   0.17374246  0.4282205  -0.6584536   0.40002584\n",
      " -0.02035541 -0.6730787  -1.0269235   0.16877276 -0.09248718 -0.79977626\n",
      "  0.38093388  0.517123    0.04200942 -0.04867543 -0.18772231  0.16339514\n",
      " -0.21974939  0.21939285  0.03676519 -0.29750264 -0.37409618 -0.5209507\n",
      " -0.41314626 -0.48947737 -0.8189656   0.08531463  0.34576967  0.12506004\n",
      "  0.2494527  -0.25254673 -0.03156092  0.27573147 -0.60857177  0.33569965\n",
      "  0.22913168  0.660708   -0.30215782 -0.05315326  0.22247535  0.06138686\n",
      "  0.3355515  -0.08485195  0.08764572  0.10872045 -0.40389323 -0.14949791\n",
      "  0.1945848  -0.8106063   0.7973094  -0.41162536  0.01364149  0.23472941\n",
      " -0.09732249 -0.29044062  0.03843206 -0.07090483 -0.1740449  -0.4485937\n",
      " -0.3186723   0.41656074 -0.05431665  0.14036216  1.0559162   0.53018135]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./test_data/folksam_report_mid_2019.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document_for_embedding(pdf_path):\n",
    "    document = pdf_to_text(pdf_path)\n",
    "    sentences = sent_tokenize(document)\n",
    "    result = []\n",
    "    for s in sentences:\n",
    "        cleaned = s.replace(\"-\\n\", \"\")\n",
    "        cleaned = cleaned.replace(\"\\n\", \" \")\n",
    "        cleaned = cleaned.replace(\"\\x0c\", \"\")\n",
    "        cleaned = cleaned.replace(\"\\t\", \" \")\n",
    "        cleaned = cleaned.replace(\"  \", \" \")\n",
    "        result.append(cleaned)\n",
    "    return result\n",
    "\n",
    "sentences = clean_document_for_embedding(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Folksam business performance.\"]\n",
    "\n",
    "embedded_query = model.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.similarity_functions import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interim Report January - June 2019 Q1 Q2 Q3 Q4 The Folksam Group 2019 1 Overview: The Folksam Group Folksam overall Folksam Customer Index (FCI), % Premiums, SEK million1 Assets under management, at period-end, SEK million 2,3,4 Unit-linked insurance assets, at period-end, SEK million3,4,5 Number of full-time positions6 Jan – Jun 2019 747 35,051 441,124.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "HIGH SIMILARITY 0.5282776\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2)  Konsumentkooperationens Pensionsstiftelse is not included.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.11705731\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The blue bond was issued by the Nordic Investment Bank (NIB) and is a Baltic Blue Bond, which is used for investments aimed at managing and protecting water resources in the Baltic region.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.04079213\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The green bond with mortgages as collateral was issued by SBAB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.061128054\n",
      "----------------------------------------------------------------------------------------------------\n",
      "• In the beginning of May, KPA Pension signed an agreement to acquire a newly built office property in the Globen area of Stockholm for just over SEK 1.6 billion.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.043165892\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The acquisition is part of increasing KPA Pension’s property assets.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.005051507\n",
      "----------------------------------------------------------------------------------------------------\n",
      "She replaces Harriet Pontán who has retired.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.03663068\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2 Three brands, two groups and eight insurance companies Folksam General Saco Folksam  Försäkring 51%* Tre Kronor Folksam Life Folksam  Unit-Linked Insurance KPA  Pension** 60%* Folksam  51%* LO Fondförsäkring * Refers to the Folksam Group’s participating interest.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "HIGH SIMILARITY 0.50741786\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In this interim overview, we present reports on Folksam as a whole, the Folksam General Group and Folksam Life Group.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "HIGH SIMILARITY 0.5530148\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Jens Henriksson President and CEO 4 Market overview and the Folksam Group Market trends Macroeconomics The industry statistics presented by Swedish insurance at the end of May show that Folksam is taking market shares in both general and life insurance operations.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "HIGH SIMILARITY 0.50057685\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Continued escalation of trade conflicts may involve more countries, create more uncertainty and thus have greater repercussions on global growth.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.008338589\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The form of the UK's exit from the EU continues to create uncertainty about the economic development of the country and to some extent the rest of Europe.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.07658451\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At 30 June 2019, the unit-linked insurance assets amounted to SEK 163,580 million (150,229).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.035658717\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Folksam Life Group’s various companies exhibited a relatively good return during the first six months of the year, compared with the previous year.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "HIGH SIMILARITY 0.6033535\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The act and the regulations will come into force on 1 December 2019.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.037828468\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Transfer rights The government submitted a bill on a more effective transfer right of insurance savings on 29 May.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.058935717\n",
      "----------------------------------------------------------------------------------------------------\n",
      "9 Folksam Life (Jan-Jun): • In June, the union ST chose Folksam as the new supplier for its members insurances.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "HIGH SIMILARITY 0.50216496\n",
      "----------------------------------------------------------------------------------------------------\n",
      "•  Folksam entered into an agreement with the broker Söder berg & Partners regarding becoming a selectable alternative in the company’s Småbolagsplan Modern procurement, which is targeted at client companies with between 1 and 29 insurable employees.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "HIGH SIMILARITY 0.58339274\n",
      "----------------------------------------------------------------------------------------------------\n",
      "• Jonas Nordling resigned from the board on 1 February at his own request.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.097274385\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The value amounted to just over SEK 9 billion.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LOW SIMILARITY -0.020589313\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for idx, embedded_document_sentence in enumerate(embeddings):\n",
    "    s = cosine_similarity(embedded_document_sentence, embedded_query[0])\n",
    "    if s > 0.5:\n",
    "        print(sentences[idx])\n",
    "        print(\"-\" * 100)\n",
    "        print(\"HIGH SIMILARITY\", s)\n",
    "        print(\"-\" * 100)\n",
    "    elif s < 0:\n",
    "        print(sentences[idx])\n",
    "        print(\"-\" * 100)\n",
    "        print(\"LOW SIMILARITY\", s)\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to compute embeddings for all documents in a dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(path_to_files, embedding_model, output_path=\"./test_data/embeddings.json\"):\n",
    "    \n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for file in os.listdir(path_to_files):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".pdf\"): \n",
    "            preprocessed_document = clean_document_for_embedding(os.path.join(path_to_files, filename))\n",
    "            sentence_embeddings = embedding_model.encode(preprocessed_document)\n",
    "            result.append({\"file_name\": filename, \"embeddings\": sentence_embeddings.tolist()})\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "test_path = \"./test_data\"\n",
    "compute_embeddings(test_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"./test_data/embeddings.json\"\n",
    "\n",
    "with open(embeddings_path) as f:\n",
    "    embeddings_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement sentence-document similarity as average of k most similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swedbank_mortgage_2018.pdf 0.4944541973397848\n",
      "--------------------------------------------------\n",
      "folksam_report_mid_2019.pdf 0.45705148123363043\n",
      "--------------------------------------------------\n",
      "seb_no_1901_en.pdf 0.6164372044735378\n",
      "--------------------------------------------------\n",
      "swedbank_annual_2017.pdf 0.5603944852043308\n",
      "--------------------------------------------------\n",
      "danske_bank_Outlook_January 2019.pdf 0.5956994259024004\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.similarity_functions import cosine_similarity\n",
    "\n",
    "def semantic_similarity(document_embeddings, query_embedding, k=5):\n",
    "    similarities = [cosine_similarity(doc_sent, query_embedding) for doc_sent in document_embeddings]\n",
    "    return sum(sorted(similarities)[-k:]) / k\n",
    "\n",
    "\n",
    "query = model.encode([\"Scandinavian economic outlook in 2017\"])[0]\n",
    "\n",
    "for doc in embeddings_json:\n",
    "    print(doc[\"file_name\"], semantic_similarity(doc[\"embeddings\"], query))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
