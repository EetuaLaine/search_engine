{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is for developing and experimenting with functions related to the search engine project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse a .pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(input_file):\n",
    "    i_f = open(input_file,'rb')\n",
    "    resMgr = PDFResourceManager()\n",
    "    retData = io.StringIO()\n",
    "    TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())\n",
    "    interpreter = PDFPageInterpreter(resMgr,TxtConverter)\n",
    "    for page in PDFPage.get_pages(i_f):\n",
    "        interpreter.process_page(page)\n",
    " \n",
    "    txt = retData.getvalue()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess parsed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "\n",
    "def preprocess_document(document):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    stop_words = stopwords.words('english')\n",
    "    number_token_name = \"special_number_token\"\n",
    "    \n",
    "    tokens = word_tokenize(document)\n",
    "    tokens = [t for t in tokens if t.isalnum() and len(t) > 1 and t not in stop_words]\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    tokens = [NUMBER_TOKEN_NAME if t.isnumeric() else t for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine vocabulary\n",
    "\n",
    "Sometimes it occurs that words get cut into two tokens because there is a dash and a newline somewhere in the middle of the word. This results in some noisy tokens being created. Additionally, there are several tokens, which contain both numeric and alpha characters. These could be further examined and parsed with some regex manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-aefb5b0756d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Corpus size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(set(tokens)), \"Corpus size:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.Series(tokens).value_counts()\n",
    "\n",
    "print(counts[:60])\n",
    "print(\"-\" * 100)\n",
    "print(counts[-60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Drop the first five elements to better see the distribution of tokens\n",
    "plt.hist([counts.values[5:]], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring together the components into a function that preprocesses a pdf document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pdf_document(pdf_path, stemmer=None, stop_word_list=None, NUMBER_TOKEN_NAME=None):\n",
    "    # Parse pdf\n",
    "    parsed_text = pdf_to_text(pdf_path)\n",
    "    \n",
    "    if stemmer is None:\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "    \n",
    "    if stop_words_list is None:\n",
    "        stop_words_list = stopwords.words('english')\n",
    "    \n",
    "    if NUMBER_TOKEN_NAME is None:\n",
    "        NUMBER_TOKEN_NAME = \"special_number_token\"\n",
    "\n",
    "    preprocessed_document = preprocess_document(parsed_text)\n",
    "    \n",
    "    return preprocessed_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple index for a document\n",
    "\n",
    "This function creates an index for a list, which is essentially a dictionary containing the counts for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(preprocessed_document):\n",
    "    return pd.Series(preprocessed_document).value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine to obtain a function, which indexes a pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_pdf_document(input_pdf):\n",
    "    preprocessed_file = preprocess_pdf_document(input_pdf)\n",
    "    return create_index(preprocessed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity between two indices\n",
    "\n",
    "This function computes the intersection of the value counts divided by the minimum sum of value counts. Under the assumption that one index is significantly smaller than the two, this should quickly give a score based on how many of the token occurrences in the smaller document are found in the larger document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_similarity(idx1, idx2):\n",
    "    sum_1 = sum(idx1.values())\n",
    "    sum_2 = sum(idx2.values())\n",
    "    \n",
    "    intersection = 0\n",
    "    \n",
    "    if sum_1 < sum_2:\n",
    "        min_idx = idx1\n",
    "        max_idx = idx2\n",
    "        min_sum = sum_1\n",
    "    else:\n",
    "        min_idx = idx2\n",
    "        max_idx = idx1\n",
    "        min_sum = sum_2\n",
    "    \n",
    "    for k, v in min_idx.items():\n",
    "        intersection += min(max_idx.get(k, 0), v)\n",
    "    \n",
    "    return intersection / min_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over pdf documents, compute indices and write them to a json file as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_indices(path_to_files, output_path=\"./test_data/indices.json\"):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for file in os.listdir(path_to_files):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".pdf\"): \n",
    "            index = index_pdf_document(os.path.join(path_to_files, filename))\n",
    "            result.append({\"file_name\": filename, \"index\": index})\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "test_path = \"./test_data\"\n",
    "compute_indices(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write function that retrieves the indices from the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_indices_from_json(indices_path=None):\n",
    "    try:\n",
    "        with open(indices_path) as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"JSON loading failed with exception:\", e)\n",
    "        return None\n",
    "\n",
    "json_path = \"./test_data/indices.json\"\n",
    "    \n",
    "    \n",
    "indices = read_indices_from_json(indices_path=json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the most relevant documents given a query string\n",
    "\n",
    "This function takes the similarity function as an argument, and also takes a function as an argument which retrieves the documents in a list where elements are in the form {\"file_name\": example_file.pdf, \"index\": {\"example\": 1, \"token\": 1}}. This way the indices can later be obtained from eg. local json files, an S3 bucket or MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swedbank_annual_2017.pdf',\n",
       " 'danske_bank_Outlook_January 2019.pdf',\n",
       " 'swedbank_mortgage_2018.pdf']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar_documents(query_string, compute_similarity, get_indices, n=3, \n",
    "                           indices_path=\"./test_data/indices.json\"):\n",
    "    \n",
    "    preprocessed_query = preprocess_document(query_string)\n",
    "    query_index = create_index(preprocessed_query)\n",
    "    \n",
    "    indices = get_indices(indices_path=indices_path)\n",
    "    \n",
    "    similarities = [compute_similarity(index[\"index\"], query_index) for index in indices]\n",
    "    \n",
    "    zipped = sorted([(idx, sim) for idx, sim in enumerate(similarities)], key=lambda t: -t[1])\n",
    "    \n",
    "    indices_to_return = [t[0] for t in zipped[:n]]\n",
    "        \n",
    "    return [indices[idx][\"file_name\"] for idx in indices_to_return]\n",
    "\n",
    "most_similar_documents(\"what is the revenue of danske bank\", index_similarity, read_indices_from_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
